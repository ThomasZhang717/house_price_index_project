\documentclass[12pt,a4paper]{article}
\usepackage{exscale}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{appendix}
\usepackage{threeparttable}
\usepackage{array}
\usepackage{alltt}
\usepackage{color}
\usepackage{colortbl}
\usepackage{graphicx}
\usepackage{import}
\usepackage{natbib}
\usepackage{rotating}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage[font=small,labelfont=bf,labelsep=colon]{caption}
\usepackage[hidelinks]{hyperref}
\usepackage{chngcntr}
\usepackage{lscape}
\renewcommand{\sectionautorefname}{Section}
\renewcommand{\subsectionautorefname}{Section}
\renewcommand{\equationautorefname}{Eq.{\!}}
\renewcommand{\footnoteautorefname}{Fn.{\!}}

\renewcommand{\baselinestretch}{1.5}

\setlength{\parskip}{1.5ex plus0.5ex minus0.5ex}
\setlength{\textwidth}{14cm} \setlength{\oddsidemargin}{0.95cm}

\def\trans{\top}                           % trans(x) = x^\trans
\def\E{\mbox{E}}                          % expected value
\def\V{\mbox{Var}}                      % variance
\def\C{\mathcal{C}}                      % variance
\def\N{\mbox{N}}                         % Normal distribution
\def\P{\mbox{P}}                          % Probability
\def\B{\mathcal{B}}                       % bias
\newcommand{\IF}{\boldsymbol{1}}             % indicator function
\def\defeq{\stackrel{\mathrm{def}}{=}}        % for definitions
 \newcommand{\vek}[1]{\mathbf{#1}}          %  \vek{x} generates bold x
\newcommand{\bsy}[1]{\boldsymbol{#1}}    % \bsy{}  generates bold symbol
%\DeclareMathOperator*{\argmin}{arg\,min}
%for tables cells length
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}} %left
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}   %Center
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}  %Right



\begin{document}

\title{Appendix for `Automated Valuation Modeling and Messy Data: A Case Study for Perth'}

\author{Felix Chan, Greg Costello, Rainer Schulz, and Zhuoran Zhang\thanks{Chan, Costello, and Zhang: School of Accounting, Economics and Finance, Curtin University, Building 408, Kent Street, Perth 6102, Australia; {felix.chan@cbs.curtin.edu.au}, {greg.costello@cbs.curtin.edu.au}, and {zhuoran.zhang1@postgrad.curtin.edu.au}. Schulz: University of Aberdeen Business School, Edward Wright Building, Dunbar Street, Aberdeen AB24 3QY, United Kingdom, {r.schulz@abdn.ac.uk}.\setlength{\baselineskip}{1.25em}}}

\date{\today}

\maketitle

\newpage
\tableofcontents

\newpage
\appendix

\section{Data preparation}

\subsection{The residential properties in \textit{Landgate} data}

There are 185,980 residential building transaction records, those cover 99.31\% of total buildings from 2015 to 2020. The residential buildings are selected by property classes. The summary of all properties classes in original data set presents in \autoref{tab_summary_stats_variables_propertyclass_total}. Our rules of residential building selection are rely on the definition of residential buildings, that is clarified in the Functional Classification of Building Structure 1999 (FCB, revision 2011) and Census Dictionary (2016)\footnote{These two official documents are available on the Australian Bureau of Statistics (ABS) website.}. Generally, three categories are described. The first and primary category is separate houses. The next contains semi-detached, row or terrace houses and town houses, those could be one-storey and two or more storeys. The last group includes flats, units or apartments, those are in a one or two storey block, in a three storey block, in a four or more storey block or attached to a house. The classes with one asterisk in \autoref{tab_summary_stats_variables_propertyclass_total} are the residential building types we pick.

%
\begin{quote}
\centering
[\autoref{tab_summary_stats_variables_propertyclass_total} about here.]
\end{quote}
%

\subsection{The `raw data' preparation}

The \textit{Landgate} data contains 185,980 transactions of residential properties from 2015 to 2020. Firstly, we identify 3,676 sales involving multiple properties or units\footnote{There are variables called `MULTI' and `NO OF UNITS' to locate these transactions. `MULTI' indicates several properties are sold together in one transaction. `NO OF UNITS' means the number of units included in the sold property, such as the two-storey building has 10 flats.} (``bundle sale''), as they are not relevant and the removal does not obtain bias on the overall sample \citep{krause_lipscomb16}. Then, duplicate records are detected by the identity information of properties and transactions, 3,251 copies in total\footnote{There are 6,417 duplicates, therein 3,081 properties have one copy, 85 properties have 2 copies.}. If two or more records have the same property IDs (parcel ID, PIN and land ID) and the same transaction information (transaction price, date and application number), they will be identified as duplicate records. We keep the records those have more available information, and abandon their copies. Meanwhile, there are 68 observations those are restructured or refurnished, because the year of build is larger than the year of sale. Off-plan property refer to property which is available to purchase before it has been constructed. 2,587 off-plan property sales also need to be excluded from the data, because their prices are very volatile in short period and building features are inherently not available. These sales are identified when the transactions don't have any valuable building information (only zeros or blanks in building features). In addition, the \textit{Landgate} data contain some non-market transaction records. The prices of these properties are extremely low, such as AUD \$2. Usually, these transaction could be identified by stakeholders of sales. However, due to privacy, buyers and sellers are not visible in our case. Alternatively, the yearly maximum and minimum prices of sold properties from 2015 to 2020 are set as boundaries of transaction prices. The source of this information is \textit{Australian Urban Research Infrastructure Network} (AURIN), the data provider is \textit{Australian Property Monitors} (APM)\footnote{APM, a part of the Domain Group, is one of the largest property information providers in Australia and the trusted source of property information for many leading banking institutions and property professionals across the country.}. The yearly maximums and minimums are available in five level 4 statistical areas (SA4), Perth-Inner, Perth-Northeast, Perth-Northwest, Perth-Southeast and Perth-Southwest\footnote{In dataset, only Local Government Areas (LGA) are observed. SA4s are larger areas those include several LGAs. The relationship of SA4s and LGAs in Perth metro-area are defined in \textit{Australian Bureau of Statistics} (ABS) as following. \textbf{Perth-Inner}: Cambridge, Claremont, Cottesloe, Mosman Park, Nedlands, Peppermint Grove, Perth City, Subiaco and Vincent. \textbf{Perth-Northeast}: Bayswater, Bassendean, Mundaring and Swan. Perth-Northwest: Joondalup, Stirling and Wanneroo. \textbf{Perth-Southeast}: Armadale, Belmont, Canning, Gosnells,  Kalamunda, Serpentine-Jarrahdale, South Perth and Victoria Park. \textbf{Perth-Southwest}: Cockburn, East Fremantle, Fremantle, Kwinana, Melville and Rockingham.}. 3,638 observations, the transaction prices of those fall out of the min-max intervals of their level 4 statistical areas, should be removed from the data. Finally, the `raw data' has 174,137 observations.

\subsection{The amendment of issues}

\noindent \autoref{tab_summary_stats_variables_cleaning} reports the summary of variables after removing the irrelevant observations. 

%
\begin{quote}
\centering
[\autoref{tab_summary_stats_variables_cleaning} about here.]
\end{quote}
%

\noindent Before presenting and using the `raw data', we have to fix some common issues. As introduced previously in data section, the coordinates are appended to each observation, PIN is the bridge between geographical data and transaction data. However, some properties are not assigned a PIN. Because they share land ownership with the other properties or they are new established, their geographical information are not updated or measured so far. This issue is fixed by geocoding the address of the properties that their coordinates are not available currently. Open source geocoding API and Google geocoding API are employed, their output coordinates are compared. All of the unavailable coordinates are filled.

\noindent The zeros or blanks issue is highly common in real estate property level data\footnote{Zero issue is in the numerical variables, blanks issue is in the categorical variables (* in \autoref{tab_summary_stats_variables_cleaning}).}. The principal step is to identify whether the information is actually missing or if the blanks simply represent the lack of positive values \citep{krause_lipscomb16}. Due to immature data management, zeros in one feature do not always mean the property doesn't have the feature. It also means that the information is not available at the moment of the transaction\footnote{We confirm this from the \textit{Landgate}, our data provider. From the end of 2017, the authority has gazetted the move to paperless conveyancing. The conveyancing information could be from different sources, some of them may be updated late.}, even, some information could be updated in subsequent transactions in future. We correct this issue according to the concept of gross living area, a suggestion in \citep{krause_lipscomb16}. Building features are divided into three groups, essential characteristics, living characteristics and add-on facilities. Essential characteristics are the most fundamental features of a building including bedroom, bathroom, lounge, kitchen, wall, roof, building area and year of build. These essential features can not be zero or empty as long as the property has a building on the ground. If zero appears in these features, it can be safely assumed that the value is not available at the transaction moment. The living characteristics contain dining room, family room, game room, meal room, study room and car park. They are not that necessary as the essential characteristics to a building. The zero attributes could indicate that the building don't have these features. Add-on facilities are the features those have the least significance to residential buildings, such as swimming pool and tennis court. However, these add-ons could significantly affect the value of a building. Zeros or blanks represent no presence of add-on facilities.

\noindent In addition, we reform categorical building features to dummies. If the wall of one property is made of bricks, Brick-Wall is 1. Contrariwise, the others except blanks are set to zero. When blanks appear in wall material variable, we decide that they are missing. Because wall is one of essential characteristics. We do the same in roof material variable. However, swimming pool is an add-on facility. \citet{krause_lipscomb16} suggest that a missing or null value may signify no pool belonging to the property as opposed to a missing or unknown value. Different from what we do to wall and roof, we reform ``B/G'' and ``A/G''\footnote{``B/G'' and ``A/G'' are two types of swimming pool, those mean ``below ground'' and ``above ground''.} to 1 which means the property has a swimming pool. Blanks are set to zero directly.   

\section{Implementation of modular AVM systems}

\subsection{Description of the applied estimation models}

\subsection*{Hedonic linear model}

The hedonic pricing method is well established, it breaks down the target variable being researched into its constituent characteristics and the estimates of contribution value of each characteristics. \citet{rosen74} introduced hedonic price theory, and a general linear form could be written as:
\begin{equation}
Y = \beta \cdot X + \varepsilon
\end{equation}   
where $\beta$ means the vector of unit price of each characteristics, $X$ represents the matrix of characteristics of properties. In real estate, hedonic pricing method is commonly used to research properties those are heterogeneous. Each property is unique in the world, and they have variety of characteristics combination. However, their characteristics are easily measured, which make properties more amenable to hedonic pricing method than most other commodities.

\subsection*{Decision tree}

Decision tree is the fundamental tree-based model, that can be applied to both classification and regression problem\footnote{In this article, decision tree is for solving regression problem. The description introduces the characteristics of decision tree when it builds a regression model. For classification problem, the description will be slightly different.}. The process of growing a decision tree could be roughly summarized by two steps \citep{James2017}:
\begin{enumerate}
\item The predictor space -- the set of possible values for $P$ predictors, $X_1, X_2,$ $\cdots, X_p, \cdots, X_P$ -- is divided into $J$ distinct and non-overlapping regions, $R_1, R_2, \cdots, R_j, \cdots, R_J$.
\item The prediction of observations in region $R_j$ is simply the mean of the response values for all training observations in $R_j$. 
\end{enumerate}
In the fundamental work of \citet{Breiman1984} several split criteria are suggested. For continuous responses, the residual of sum squares (RSS) is a popular choice. The goal of decision tree is to find regions $R_1, R_2, \cdots, R_j, \cdots, R_J$ that minimize the RSS:
\begin{equation}
\sum_{j=1}^J \sum_{i \in R_j} (y_i - \hat{y}_{R_j})^2
\end{equation}
where the $\hat{y}_{R_j}$ is the mean response for the training observations falling in the region $R_j$. However, it is computationally infeasible to consider all possible partition of the predictor space into $J$ regions. Because of this, a top-down, greedy approach, also known as recursive binary partitioning. The approach starts at the top of the tree and then successively splits the predictor space; each split creates two new branches and the best one is made at this particular node. Then, the full process will be simplified to a repetitive one-node splitting process until the certain criterion is reached. The simplified one-node splitting process starts from selecting the predictor ($X_p$) and the cutpoint ($c$), such that the predictor space is divided into two regions, $R_{Left}(p,c) = \{ X|X_p < c \}$ and $R_{Right}(p,c) = \{ X|X_p \geq c \}$. In this step, all of the predictors $X_1, X_2,$ $\cdots, X_p, \cdots, X_P$, and all possible cutpoint values $c$ for each of the predictors are considered. Then, the goal is to seek the best $p$ and $c$ to minimize the equation:
\begin{equation}
\sum_{i: x_i \in R_{Left}(p,c)} (y_i - \hat{y}_{R_{Left}})^2 + \sum_{i: x_i \in R_{Right}(p,c)} (y_i - \hat{y}_{R_{Right}})^2
\end{equation}
where $\hat{y}_{R_{Left}}$ is the mean response for the training observations in $R_{Left}(p,c)$, and $\hat{y}_{R_{Right}}$ is the mean response for the training observations in $R_{Right}(p,c)$. This procedure makes trees are robust to outliers. Because feature space is divided by one line in each split. The distance between the data points and the line has a negligible effect on RSS, in contrast to ordinary least squares (OLS). A tree will keep growing until a certain criterion is reached, such as a limited number of observations in a terminal nodes (the size of leaves). An five-region two-predictor example of this approach is shown in \autoref{fig_decision_tree_example}. A more detailed description of this approach can be found in \citet{Breiman1984} and \citet{Hastie2009}.

%
\begin{center}
[\autoref{fig_decision_tree_example} about here.]
\end{center}
%

\subsection*{Random forest}

\citet{Breiman1996} invent bootstrapping aggregation (``bagging'') to improve the performance of decision tree, it is also the best method to trees when there is `noise' in data \citep{Dietterich2000}. In bagging, a number of trees are fitted to bootstrapped data, a fraction of observations in training set (with replacement). The trees are assembled, the mean of the predictions produced by each single tree are chosen as the final prediction. This is the rudiment of random forest. As an extension of bagging, random forest \citep{Breiman2001} provides a ``forest'' of decision trees, which the trees are not closely correlated with other trees in the forest. In the other words, random forest allows decision trees more independent by randomly selecting a subset of predictors in each split. When trees are constructed, a split in a tree is considered each time, a random sample of $M$ predictors is chosen as split candidates from the full set of $P$ predictors. Only one of those $M$ predictors is selected as the primary variable of the split. This means that some weak predictors will have more of chance, and strong predictors will has less effect on the response. This ensures that the trees in the ``forest'' can be more unique, and that the average of large number of trees can improve over-fitting tendency. Meanwhile, each tree is grown on a bootstrapped sample from training data set. Unusual observations will appear in only few sample sets, which will introduce additional variability and make random forest robust against outliers.

\subsection*{Gradient boosting machine}

Gradient boosting machine \citep[GBM,][]{Friedman2001,Friedman2002} is another approach to improve the predictions resulting from a decision tree. GBM also plants a ``forest'' of trees as random forest. In random forest, each tree is grown on a bootstrapped data, independent from the other trees. In GBM, however, each tree is grown sequentially using information from previously grown trees on a fraction of training set (without replacement). The trees are in the same ``family line''. The GBM fitting procedure is also different from random forest. Firstly, it sets the predict values equal to zero, the residuals are equal to the value of dependent variable of observations in the training set.
\begin{equation}
\hat{f}(X) = 0~~and~~\varepsilon = Y - 0.
\end{equation}
Then, the number of tree ($N$) and the number of splits\footnote{Generally, the interaction depth is used, these two tuning parameter are similar.} ($d$) are pre-decided. A tree ($\hat{f^n}(X)$) is fitted with d splits using the training data.  The predict values are updated by adding in a shrunken version of the new tree.
\begin{equation}
\begin{split}
\hat{f}(X) &\leftarrow \hat{f}(X) + \lambda\hat{f^n}(X).\\
\varepsilon &\leftarrow \varepsilon - \lambda\hat{f^n}(X).
\end{split}
\end{equation}
After repeating $N$ times, the final output predict values are
\begin{equation}
\hat{f}(X) = \sum_{n=1}^N \lambda\hat{f^n}(X).
\end{equation}
At this moment, the residuals are stable and minimized depending on current available training data set and tuning parameters. Differing from random forest's averaging process, GBM applies an updating algorithm.

\subsection{Complementary systems}

Different from the bundle approaches and stand-alone approaches, those modules work simultaneously or modules cooperate in a designed sequence, the package approaches are a hybrid. In package approaches, if modules could concurrently run, then they are tied together as a package. The package work with the rest modules, those can not concurrently run, in a pre-defined sequence.

\noindent $D4$, $R4$ and $G4$ impute missing values in training sets firstly by multiple imputation. Then, outliers and estimation are handled by a packaged module. In contrast, $D5$, $R5$ and $G5$ follow an opposite path. They initially clean outliers in training sets, then packaged module deals with missing values and estimation. These six systems separately belong to decision tree family, random forest family and gradient boosting machine family for the purpose of complementary comparison. The full process map of seventeen systems are depicted completely in \autoref{fig_approach_tools_all_complement}.  

%
\begin{quote}
\centering
[\autoref{fig_approach_tools_all_complement} about here.] 
\end{quote}
%

\subsection{Tuning parameters}

The hyper-parameters of decision tree, random forest and GBM are tuned by grid searching on ten fold cross validations. The optimal hyper-parameters are selected in compliance with one standard error rule (recommendation in \citet{Hastie2009}, Section 7.10), the most parsimonious model within one standard error of the minimum of evaluation metrics is chosen. One standard error rule is a conservative way to select the optimal hyper-parameters. But this could ensure that the model is still in the top tier of accuracy when training set is slightly modified. \autoref{tab_summary_tune_parameters} reports the tuning parameters for each of three statistical model families. Models in on family share the same hyper-parameters.

%
\begin{quote}
\centering
[\autoref{tab_summary_tune_parameters} about here.] 
\end{quote}
%

\subsection{\textsf{R} packages}

We implement the thirteen approaches, those combine build-in techniques and external methods, using various R packages, see \autoref{tab_summary_r_packages}. \textsf{stats} \citep{RCT2021}, \textsf{rpart} \citep{Therneau2019}, \textsf{gbm} \citep{Greenwell2020} and \textsf{isotree} \citep{Cortes2021} are common used \textsf{R} packages to linear model, implement decision tree, GBM and isolation forest. We apply two packages in random forest family. \textsf{rfsrc} \citep{Ishwaran2021} is to achieve on-the-fly imputation in random forest, it is used in approach $C5$ and $C6$, \textsf{ranger} \citep{Wright2017} is used in approach $C7$ and $C8$, due to its high computational efficiency. We set all main parameters with the same values for avoiding the accuracy difference from these two packages. \textsf{missRanger} \citep{Mayer2021} is an alternative implementation of the `missForest' algorithm using \textsf{ranger} to build chaining random forests. All packages can be downloaded from \href{https://cran.r-project.org/web/packages/available_packages_by_name.html}{https://cran.r-project.org/}.

%
\begin{quote}
\centering
[\autoref{tab_summary_r_packages} about here.] 
\end{quote}
%

\section{Additional Results}


\newpage
\bibliographystyle{kluwer}
\bibliography{avm_perth.bib}
\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%
%%% TABLES START HERE %%%
%%%%%%%%%%%%%%%%%%%%%%%%%

\setcounter{table}{0}
\renewcommand{\thetable}{A\arabic{table}}
\clearpage
\input{./tables/tab_summary_stats_variables_propertyclass_total}

\clearpage
\input{./tables/tab_summary_stats_variables_cleaning}

\setcounter{table}{0}
\renewcommand{\thetable}{B\arabic{table}}
\clearpage
\begin{landscape}
\input{./tables/tab_summary_tune_parameters}
\end{landscape}

\clearpage
\input{./tables/tab_summary_r_packages}

\setcounter{table}{0}
\renewcommand{\thetable}{C\arabic{table}}
\clearpage
\input{./tables/tab_summary_combinations_results_complements}

%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% FIGURES START HERE %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%

\setcounter{figure}{0}
\renewcommand{\thefigure}{B\arabic{figure}}
\clearpage
\import{./figures/}{fig_decision_tree_example}

\clearpage
\import{./figures/}{fig_approach_tools_all_complement}

\end{document}